<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ML Lecture - Convolutional Neural Network</title>
    <url>/2021/03/02/Convolutional-Neural-Network/</url>
    <content><![CDATA[<h6 id="tags-Hung-yi-Lee-ML-Lecture"><a href="#tags-Hung-yi-Lee-ML-Lecture" class="headerlink" title="tags: Hung-yi Lee ML Lecture"></a>tags: <code>Hung-yi Lee ML Lecture</code></h6><h2 id="ML-Lecture-Convolutional-Neural-Network"><a href="#ML-Lecture-Convolutional-Neural-Network" class="headerlink" title="ML Lecture - Convolutional Neural Network"></a>ML Lecture - Convolutional Neural Network</h2><h3 id="How-Neural-Network-classify-image"><a href="#How-Neural-Network-classify-image" class="headerlink" title="How Neural Network classify image?"></a>How Neural Network classify image?</h3><p>在原先的DNN中，每個neuron就是在做不同的特徵分類，每層要進行的分類工作會越來越複雜(同Modularization)。</p>
<a id="more"></a>
<p><img src="https://i.imgur.com/Qw1N7D1.png"></p>
<p>而在做影像分類時，每個neuron只需觀察圖像中的一小部分是否含有某特徵即可，不須看完整張圖。<br><img src="https://i.imgur.com/atBK7sP.png"></p>
<p>因為每個張圖含有某特徵的位置都不同，所以辨識同一特徵的neuron可共享同樣的參數。<br><img src="https://i.imgur.com/SFWt7lx.png"></p>
<p>而我們可利用Subsampling將圖片縮小，如此也可減少參數計算量。</p>
<ul>
<li>Subsampling：二次抽樣，如只取圖片的基數行、偶數列，如此可將圖片縮小。</li>
</ul>
<p><img src="https://i.imgur.com/MJ8ThrL.png"></p>
<h3 id="How-CNN-work"><a href="#How-CNN-work" class="headerlink" title="How CNN work?"></a>How CNN work?</h3><p>整個CNN就分成Convolution、Max Pooling 及 Flatten這三步驟。<br><img src="https://i.imgur.com/EuUz0TJ.png"></p>
<p>在使用CNN進行分類前，需注意：</p>
<ul>
<li>圖片中每個圖案(特徵)是很小的</li>
<li>每個圖案會出現在不同圖的不同區域</li>
<li>Subsampling不會改變原圖</li>
</ul>
<p><img src="https://i.imgur.com/N1UQQ7j.png"></p>
<h4 id="CNN-Convolution"><a href="#CNN-Convolution" class="headerlink" title="CNN-Convolution"></a>CNN-Convolution</h4><p>每一個Convolution都會有很多個Filter，每一個Filter都是一個Matrix，代表一張image裡的某一區塊是否含有此一特徵。而Filter裡面的值就是神經網路中的學習參數。</p>
<p>Filter透過stride(步伐)大小逐步在圖片中移動，並與覆蓋的位置做內積。<br>全部遊走一遍後可得一特徵圖(feature map)，即代表圖中的哪個部分含有該Filter的特徵。<br><img src="https://i.imgur.com/yErRHqJ.png"><br><img src="https://i.imgur.com/ITxekAY.png"></p>
<p>Convolution就是將Fully Connected將部份weight拿掉而已:</p>
<ul>
<li>將圖片展開</li>
<li>Filter與對應位置做內積運算<ul>
<li><p>如3就是 Filter1 和第1、2、3、7、8、9、13、14、15位置內積</p>
<p>因為Convolution只需連接Filter所對應的位置(如圖例就是9個位置)，故參數計算相對Fully Connected少。<br><img src="https://i.imgur.com/YnExjoA.png"><br><img src="https://i.imgur.com/KONuGLO.png"></p>
</li>
</ul>
</li>
</ul>
<h4 id="CNN-Max-Pooling"><a href="#CNN-Max-Pooling" class="headerlink" title="CNN-Max Pooling"></a>CNN-Max Pooling</h4><p>類似Subsampling，將feature map分群，並在每群中取最大值，得一較小的圖片(Matrix)。<br><img src="https://i.imgur.com/JEr7Ray.png"><br><img src="https://i.imgur.com/uvzi1W6.png"></p>
<h4 id="CNN-Flatten"><a href="#CNN-Flatten" class="headerlink" title="CNN-Flatten"></a>CNN-Flatten</h4><p>將Matrix展開，變一維。如此即可輸入給Fully Connected Network做最後分類。<br><img src="https://i.imgur.com/c2UmQEZ.png"></p>
<h3 id="What-does-CNN-learn？"><a href="#What-does-CNN-learn？" class="headerlink" title="What does CNN learn？"></a>What does CNN learn？</h3><p>為了分析CNN到底學到了甚麼，我們挑一個Filter，並定義其啟動函式來判定輸入(input)與這Filter有多相近。</p>
<p>想了解filter的作用，我們找一張image(X)，它可以讓這個filter被啟動的程度最大。利用梯度上升(gradient ascent)來最大化啟動函數。<br><img src="https://i.imgur.com/RnN63Qv.png"><br><img src="https://i.imgur.com/z0KcOap.png"></p>
<p>而後面的fully connected也可以以同樣的方式可視化。<br><img src="https://i.imgur.com/auqaqF0.png"></p>
<p>將輸出層做可視化，發現結果與預期大不相同，但分類的結果卻是準確的(將8的照片做為輸入，模型確實預測為8)。代表CNN辨別圖片的方式與人不同。<br><img src="https://i.imgur.com/iMYc5mX.png"></p>
<p>我們在找一個$x$讓 $y^i$ 最大化，相對的要讓x的元素加總最小，意思是我們希望找一個image大部份是白的，沒有塗上筆劃(黑)，以這個概念重新可視化之後整體的呈現就不同了。<br><img src="https://i.imgur.com/jcJirrb.png"></p>
<h3 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h3><p>放大CNN所看的東西，將其視覺化並與原圖融合。<br>即取某層layer的Filter或fully connected的權重，將正項放大、負向縮小(正者越正，負者越負)。<br><img src="https://i.imgur.com/b3j5ulW.png"><br><img src="https://i.imgur.com/oNsqZLT.png"></p>
<h4 id="Deep-Style"><a href="#Deep-Style" class="headerlink" title="Deep Style"></a>Deep Style</h4><ol>
<li>將image輸入CNN得到一輸出表該image的內容。</li>
<li>將風格照輸入CNN得到一輸出表該image的風格。</li>
<li>隨機初始化一張照片，讓內容像左邊照片，風格像右邊照片。</li>
</ol>
<p><img src="https://i.imgur.com/GnbY3dZ.png"></p>
<h3 id="More-Application-Text"><a href="#More-Application-Text" class="headerlink" title="More Application:Text"></a>More Application:Text</h3><p>將輸入句的每個字轉成字向量並組成一序列矩陣，利用CNN得到該序列矩陣的feature map，並取Pooling即可將該句子分類。<br><img src="https://i.imgur.com/GQipP4x.png"></p>
]]></content>
  </entry>
  <entry>
    <title>ML Lecture - Classification &amp; Logistic Regression</title>
    <url>/2021/02/16/ML-Lecture-Classification-1/</url>
    <content><![CDATA[<h6 id="tags-Hung-yi-Lee-ML-Lecture"><a href="#tags-Hung-yi-Lee-ML-Lecture" class="headerlink" title="tags: Hung-yi Lee ML Lecture"></a>tags: <code>Hung-yi Lee ML Lecture</code></h6><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>在Classfication中，我們輸入一個X，而model告訴我們X的類別<br>ex. 機器翻譯、文法偵錯、臉部表情偵測…等</p>
<a id="more"></a>
<p><img src="https://i.imgur.com/3PDDWd7.png"></p>
<p>得到寶可夢的特徵並數值化<br><img src="https://i.imgur.com/PGgqLwz.png"></p>
<h3 id="example-of-Classification-Pokemon"><a href="#example-of-Classification-Pokemon" class="headerlink" title="example of Classification(Pokemon)"></a>example of Classification(Pokemon)</h3><p><img src="https://i.imgur.com/EKEi1kG.png"></p>
<p>以二分類為例，我們可以使用Regression。<br>輸出為{1, -1}，即輸出為1到-1之間，接近1為class1，接近-1為class2。則我們以0為分界，若輸入大於0為class1，反之則為class2。<br><img src="https://i.imgur.com/yc07fRR.png"></p>
<h3 id="Why-not-Regression"><a href="#Why-not-Regression" class="headerlink" title="Why not Regression"></a>Why not Regression</h3><p>在Linear Regression中，做Binary Classification是可行的。<br>但若資料是分群的(如右圖Class1)，則會造成model收斂於紫線(為了降低誤差)。<br><img src="https://i.imgur.com/xEiD5P8.png"></p>
<h3 id="Ideal-Alternatives"><a href="#Ideal-Alternatives" class="headerlink" title="Ideal Alternatives"></a>Ideal Alternatives</h3><p>先定義一function f(x)，裡面有一g(x)。將x帶入g(x)，若g(x)&gt;0為類別1，否則為類別0。<br>定義Loss function L(f):f(x)分類錯誤的次數。<br><img src="https://i.imgur.com/cjh4lGp.png"></p>
<h3 id="以機率模型實現"><a href="#以機率模型實現" class="headerlink" title="以機率模型實現"></a>以機率模型實現</h3><p>以箱子抽球為例<br><img src="https://i.imgur.com/ting6eJ.png"></p>
<p>將Box變為class，給一個x(要分類的對象)，它屬於某個class的機率為何，我們需要知道：</p>
<ul>
<li>$P(C1)$:從class1抽出來的機率</li>
<li>$P(C2)$:從class2抽出來的機率</li>
<li>$P(x|C1)$:從class1抽出x的機率</li>
<li>$P(x|C2)$:從class2抽出x的機率</li>
</ul>
<p>有了上面四種機率，就可以計算出$P(C1|x)$的機率(x是class1的機率)<br><img src="https://i.imgur.com/giY92h3.png"></p>
<h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><p><img src="https://i.imgur.com/Qg6Q9bj.png"></p>
<p>如何判斷一個未出現在training set的data的機率?<br><img src="https://i.imgur.com/VR5KvNW.png"></p>
<p>假設從Gaussian中取出79個點，現在給一個新的點(不存在79個資料集中的新資料)，就可以利用Gaussian Distribution function來計算出抽到該點的機率。</p>
<p>以分佈來看，該點愈接近中心點被抽到的機率會愈高，離中心愈遠被抽到的機率則愈低，因此黑點，New $x$離中心有點遠，被抽到的機率就會有點小。</p>
<p>所以，如何找出$μ$與$Σ$就是一個問題，而找出它們的概念就是Maximum Likelihood。<br><img src="https://i.imgur.com/E5xklym.png"></p>
<p>任何一個Gaussian Distribution都可以找出這79個點，只是可能性(Likelihood)不同。以右上分佈為例，它sample出左下角的機率很低，但不會為0。而要找出最大的$μ$與$Σ$，就必須用Likelihood function，即這79個點的乘積。<br><img src="https://i.imgur.com/SSFqGXU.png"></p>
<h3 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h3><ul>
<li>要找出最大的function，就必須找出最大的$μ$與$Σ$，即($μ^*$, $Σ^*$)<ul>
<li>窮舉所有的$μ^*$和$Σ^*$，並找出最大的者：$μ^∗,Σ^∗$$=arg$ $\max\limits_{μ,Σ}L(μ,Σ)$</li>
</ul>
</li>
</ul>
<p>算出$μ^*$和$Σ^*$的方法就如同機率與統計的算法一樣。</p>
<p><img src="https://i.imgur.com/3T3VdBB.png"></p>
<p>有了上面的公式就可以求出兩個calss的各別$μ$與$Σ$，也就可以計算出$P(C1|X)$。<br><img src="https://i.imgur.com/zz9Iki7.png"></p>
<h3 id="Now-we-can-do-classification"><a href="#Now-we-can-do-classification" class="headerlink" title="Now we can do classification"></a>Now we can do classification</h3><p>若$P(C1|X)&gt;0.5$，這個$x$就屬於class1<br><img src="https://i.imgur.com/tquUVzm.png"></p>
<h3 id="How’s-the-results"><a href="#How’s-the-results" class="headerlink" title="How’s the results?"></a>How’s the results?</h3><p>由右上圖可明顯的看出來這二類別並未有個明顯的分界。即使考慮了額外的七個特徵(類別)，正確率也沒有明顯的提升。<br><img src="https://i.imgur.com/IspeQUG.png"></p>
<h3 id="Modifying-Model"><a href="#Modifying-Model" class="headerlink" title="Modifying Model"></a>Modifying Model</h3><p>在實務上並不會讓每個高斯模型都擁有自己的$μ$和$Σ$，而是會共享參數，藉此減少參數的計算量。常見作法是不同類別有著相同的$Σ$。<br><img src="https://i.imgur.com/zlLaB7V.png"></p>
<h3 id="Modifying-Model-2"><a href="#Modifying-Model-2" class="headerlink" title="Modifying Model-2"></a>Modifying Model-2</h3><p>調整式子，讓水系與一般系神奇寶貝擁有相同的covariance，計算它們的likelihood：</p>
<ul>
<li>$L(μ^1,μ^2,Σ)=f_{u^1,Σ}(x^1)f_{u^1,Σ}(x^2)…f_{u^1,Σ}(x^{79})f_{u^2,Σ}(x^{80})f_{u^2,Σ}(x^{81})…f_{u^2,Σ}(x^{140})$<br><img src="https://i.imgur.com/9QOa45k.png"></li>
</ul>
<h3 id="Modifying-Model-Result"><a href="#Modifying-Model-Result" class="headerlink" title="Modifying Model-Result"></a>Modifying Model-Result</h3><p>共用covariance之後，分界變為線性，這也稱為線性模型，並且考慮所有特徵之後正確率提升為73%。<br><img src="https://i.imgur.com/y92DAXi.png"></p>
<h3 id="Three-Steps-Review"><a href="#Three-Steps-Review" class="headerlink" title="Three Steps(Review)"></a>Three Steps(Review)</h3><ul>
<li>Model<ul>
<li>有P(C1),P(C2),P(x|C1),P(x|C2)四種機率分佈，這就是模型的參數，只要你選不同的μ(mean)與Σ(covariance)，就會得到不同的機率分佈模型。</li>
<li>P(C1)&gt;0.5, class=1</li>
</ul>
</li>
<li>Goodness of a function<ul>
<li>要得到最好的模型，就必須找到最大的$μ$與$Σ$，以Maximum Likelihood求得。<br><img src="https://i.imgur.com/7o8kfyX.png"></li>
</ul>
</li>
</ul>
<h3 id="Posterior-Probability"><a href="#Posterior-Probability" class="headerlink" title="Posterior Probability"></a>Posterior Probability</h3><p><img src="https://i.imgur.com/u0YNy0Q.png"></p>
<hr>
<h3 id="補充"><a href="#補充" class="headerlink" title="補充"></a>補充</h3><p>假設$μ$與$Σ$為已知，將高斯函數在$z$展開並將共同係數提出整理，得：</p>
<ul>
<li><img src="https://i.imgur.com/gJSANQ1.png"></li>
</ul>
<p>可得知$P(C1|x)=σ(z)=σ(w⋅x+b)$，由此可證明當兩類別為同一分佈時，分界線會呈線性的。<br><img src="https://i.imgur.com/TxjMw0i.png"><br>以上計算步驟太繁瑣了，能不能直接得到w及b呢?-Logistic Regression</p>
<hr>
<h2 id="Lecture-5-Logistic-Regression"><a href="#Lecture-5-Logistic-Regression" class="headerlink" title="Lecture 5 Logistic Regression"></a>Lecture 5 Logistic Regression</h2><h3 id="Step-1：Function-Set"><a href="#Step-1：Function-Set" class="headerlink" title="Step 1：Function Set"></a>Step 1：Function Set</h3><p>我們要找posteriori probability，當機率$P_{w,b}(C_{1}|x)≥0.5$則輸出$C_{1}$，否則即輸出$C_{2}$。<br><img src="https://i.imgur.com/tHR8IYW.png"><br><img src="https://i.imgur.com/6fuJ2On.png"></p>
<h3 id="Step-2-Goodness-of-a-Function"><a href="#Step-2-Goodness-of-a-Function" class="headerlink" title="Step 2: Goodness of a Function"></a>Step 2: Goodness of a Function</h3><p>設我們有任一組w,b可產生N筆Data($x^1 … x^N$)<br>與Chapter 4 進行Classification時相同，用Maximum Likelihood找w,b。<br><img src="https://i.imgur.com/Zi4bcLm.png"><br><img src="https://i.imgur.com/dxd6k0o.png"><br><img src="https://i.imgur.com/KM2cR5B.png"><br><img src="https://i.imgur.com/wzQBvmf.png"><br><img src="https://i.imgur.com/al3Xu4E.png"><br><img src="https://i.imgur.com/CPdoOEM.png"><br><img src="https://i.imgur.com/kHgsogx.png"></p>
<h3 id="Comparison－Logistic-v-s-Linear"><a href="#Comparison－Logistic-v-s-Linear" class="headerlink" title="Comparison－Logistic v.s. Linear"></a>Comparison－Logistic v.s. Linear</h3><p>我們可以發現，在更新參數$w_{i}$ Logistic 和 Linear 是一樣的，差別在於輸出(紫色線部分)。前者輸出介於0到1之間，後者則可為任何數值。<br><img src="https://i.imgur.com/xCye0YO.png"></p>
<h3 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h3><p>使用Regression分類就是找到一條直線去分類資料<br><img src="https://i.imgur.com/SlO2fj3.png"><br><img src="https://i.imgur.com/eHP27ys.png"><br>x1,x2為座標點，-0.5為bias $b$<br><img src="https://i.imgur.com/fbKKoXb.png"></p>
<h3 id="XOR-Problem"><a href="#XOR-Problem" class="headerlink" title="XOR Problem"></a>XOR Problem</h3><p>但今天資料為分散的，就難以以一條線去做簡單分類。<br><img src="https://i.imgur.com/vLamyCp.png"><br><img src="https://i.imgur.com/KEbg8BQ.png"><br>這時我們就可將多一點的Regression疊在一起，輸出時再多加一層Regression來分類。<br><img src="https://i.imgur.com/YW2E6Lb.png"></p>
<h3 id="That’s-Deep-Learning"><a href="#That’s-Deep-Learning" class="headerlink" title="That’s Deep Learning"></a>That’s Deep Learning</h3><p><img src="https://i.imgur.com/fl5PAeq.png"></p>
<hr>
<h2 id="補充：Perceptron-v-s-Logistic-Regression"><a href="#補充：Perceptron-v-s-Logistic-Regression" class="headerlink" title="補充：Perceptron v.s. Logistic Regression"></a>補充：Perceptron v.s. Logistic Regression</h2><p>傳統的Perceptron使用step function當作輸出。<br><img src="https://i.imgur.com/G01lWle.png"><br>因step function為非線性，難以用微分計算誤差，故改用Sigmoid $σ(in)$。<br><img src="https://i.imgur.com/m7una26.png"></p>
<hr>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/02/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. </p>
<a id="more"></a>
<p>Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
